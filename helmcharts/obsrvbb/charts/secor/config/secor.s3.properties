# include=secor.properties



# ############
# # MUST SET #
# ############

# # Name of the s3 bucket where log files are stored.
# secor.s3.bucket={{ $.Values.s3_bucket_name }}

# # AWS authentication credentials.
# # Leave empty if using IAM role-based authentication with s3a filesystem.
# aws.access.key={{ $.Values.s3_access_key }}
# aws.secret.key={{ $.Values.s3_secret_id }}

# # AWS region or endpoint. region should be a known region name (eg.
# # us-east-1). endpoint should be a known S3 endpoint url. If neither
# # are specified, then the default region (us-east-1) is used. If both
# # are specified then endpoint is used.
# #
# # Only apply if the the S3UploadManager is used - see
# # secor.upload.manager.class.
# #
# # http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region
# aws.region={{ $.Values.s3_region }}
# aws.endpoint={{ $.Values.s3_endpoint }}

# # Toggle the AWS S3 client between virtual host style access and path style
# # access. See http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html
# aws.client.pathstyleaccess={{ $.Values.s3_path_style_access }}

# # Class that will manage uploads. Default is to use the hadoop
# # interface to S3.
# # secor.upload.manager.class=com.pinterest.secor.uploader.GSUploadManager
# secor.upload.manager.class=com.pinterest.secor.uploader.S3UploadManager

# # S3 path where sequence files are stored.
# secor.s3.path={{ .base_path }}

# # Output file pattern excluding prefix. Defaults to topic/partition/generation_kafkaPartition_fmOffset.gz.
# # Available placeholders are 
# # topic - The topic name the data is being fetched
# # partition - The partition name
# # generation - Generation
# # kafkaPartition - The kafka partition
# # fmOffset - First Message offset in the file.
# # randomHex - A 4 character random hex to append to the file name
# # currentTimestamp - Time of upload in epoch format
# # currentTime - Time of upload in HH-mm format
# # currentDate - Time of upload in YYYYMMDD format
# # folder - Folder to use based on message id map lookup
# secor.s3.output_file_pattern={{ .output_file_pattern }}

# ################
# # END MUST SET #
# ################
include=secor.properties

############
# MUST SET #
############

# Name of the Google Cloud Storage (GCS) bucket where log files are stored.
secor.gcs.bucket={{ $.Values.gcp_public_bucket_name }}

# GCP authentication credentials.
# Authentication will use a service account. This can be done by providing the path to the service account key file.
google.cloud.auth.service.account.enable=true
google.cloud.auth.service.account.keyfile= /Users/mansi/Downloads/sunbird-morocco-sandbox-434709-c0322f0c3ef4 (1).json

# Optionally, use values from Helm for service account details.
# google.cloud.auth.service.account.name={{ $.Values.gcp_account }}
# google.cloud.auth.service.account.key={{ $.Values.gcp_secret }}

# Class that will manage uploads for Google Cloud Storage.
secor.upload.manager.class=com.pinterest.secor.uploader.GSUploadManager

# GCS path where sequence files are stored.
secor.gs.path=gs://{{ $.Values.gcp_public_bucket_name }}/{{ .base_path }}

# Output file pattern excluding prefix. Defaults to topic/partition/generation_kafkaPartition_fmOffset.gz.
# Available placeholders:
# topic - The topic name the data is being fetched
# partition - The partition name
# generation - Generation
# kafkaPartition - The Kafka partition
# fmOffset - First Message offset in the file.
# randomHex - A 4 character random hex to append to the file name
# currentTimestamp - Time of upload in epoch format
# currentTime - Time of upload in HH-mm format
# currentDate - Time of upload in YYYYMMDD format
# folder - Folder to use based on message id map lookup
secor.gcs.output_file_pattern={{ .output_file_pattern }}

################
# END MUST SET #
################
